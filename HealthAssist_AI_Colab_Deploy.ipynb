{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# HealthAssist AI with IBM Granite 3.3-2b-instruct\n",
    "\n",
    "This notebook deploys a complete healthcare AI assistant using IBM's Granite 3.3-2b-instruct model with Streamlit interface.\n",
    "\n",
    "## Features:\n",
    "- AI-powered medical chat assistant\n",
    "- Symptom checker and analysis\n",
    "- Wellness dashboard with health analytics\n",
    "- Patient management system\n",
    "- Smart notifications\n",
    "- Role-based authentication (Admin, Doctor, Patient)\n",
    "\n",
    "## Requirements:\n",
    "- T4 GPU runtime (recommended)\n",
    "- Hugging Face account for model access\n",
    "\n",
    "**⚠️ Important: Switch to GPU runtime before running (Runtime → Change runtime type → GPU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q streamlit torch transformers accelerate bitsandbytes\n",
    "!pip install -q plotly pandas numpy datetime hashlib\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print(\"✅ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Ngrok for Public Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ngrok for public URL access\n",
    "from pyngrok import ngrok\n",
    "import getpass\n",
    "\n",
    "# Get ngrok auth token (sign up at https://ngrok.com for free)\n",
    "print(\"Get your free ngrok auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "ngrok_token = getpass.getpass(\"Enter your ngrok auth token: \")\n",
    "ngrok.set_auth_token(ngrok_token)\n",
    "\n",
    "print(\"✅ Ngrok configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Application Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('pages', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('.streamlit', exist_ok=True)\n",
    "\n",
    "print(\"✅ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit configuration\n",
    "config_content = '''\n",
    "[server]\n",
    "headless = true\n",
    "address = \"0.0.0.0\"\n",
    "port = 8501\n",
    "\n",
    "[theme]\n",
    "primaryColor = \"#1f77b4\"\n",
    "backgroundColor = \"#ffffff\"\n",
    "secondaryBackgroundColor = \"#f0f2f6\"\n",
    "textColor = \"#262730\"\n",
    "'''\n",
    "\n",
    "with open('.streamlit/config.toml', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"✅ Streamlit configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IBM Granite model utility\n",
    "model_utils_content = '''\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class IBMGraniteModelManager:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load IBM Granite 3.3-2b-instruct model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading {self.model_name} on {self.device}...\")\n",
    "            \n",
    "            # Configure quantization for memory efficiency\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load model with quantization\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            print(\"✅ IBM Granite model loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_response(self, prompt, max_length=512, temperature=0.7):\n",
    "        \"\"\"Generate response using IBM Granite model\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"Model not loaded. Please load the model first.\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            return response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def medical_chat(self, user_message, chat_history=None):\n",
    "        \"\"\"Handle medical chat conversations\"\"\"\n",
    "        system_prompt = \"\"\"You are a helpful medical AI assistant. Provide informative, accurate medical information while always recommending users consult healthcare professionals for serious concerns. Be empathetic and clear in your responses.\"\"\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        context = f\"System: {system_prompt}\\\\n\\\\n\"\n",
    "        if chat_history:\n",
    "            for msg in chat_history[-3:]:  # Last 3 messages for context\n",
    "                context += f\"{msg['role']}: {msg['content']}\\\\n\"\n",
    "        \n",
    "        context += f\"Human: {user_message}\\\\nAssistant:\"\n",
    "        \n",
    "        return self.generate_response(context, max_length=400)\n",
    "    \n",
    "    def symptom_analysis(self, symptoms, patient_info=\"\"):\n",
    "        \"\"\"Analyze symptoms and provide medical insights\"\"\"\n",
    "        prompt = f\"\"\"As a medical AI assistant, analyze these symptoms and provide helpful insights:\n",
    "\n",
    "Symptoms: {symptoms}\n",
    "Patient Info: {patient_info}\n",
    "\n",
    "Please provide:\n",
    "1. Possible conditions to consider\n",
    "2. Recommended actions\n",
    "3. When to seek medical attention\n",
    "4. General care advice\n",
    "\n",
    "Important: This is for informational purposes only. Always consult a healthcare professional for proper diagnosis.\n",
    "\n",
    "Analysis:\"\"\"\n",
    "        \n",
    "        return self.generate_response(prompt, max_length=500)\n",
    "    \n",
    "    def wellness_insights(self, health_data):\n",
    "        \"\"\"Generate wellness insights from health data\"\"\"\n",
    "        prompt = f\"\"\"As a wellness AI assistant, analyze this health data and provide personalized insights:\n",
    "\n",
    "{health_data}\n",
    "\n",
    "Please provide:\n",
    "1. Overall health assessment\n",
    "2. Areas for improvement\n",
    "3. Personalized recommendations\n",
    "4. Lifestyle suggestions\n",
    "\n",
    "Insights:\"\"\"\n",
    "        \n",
    "        return self.generate_response(prompt, max_length=400)\n",
    "'''\n",
    "\n",
    "with open('utils/model_utils.py', 'w') as f:\n",
    "    f.write(model_utils_content)\n",
    "\n",
    "print(\"✅ IBM Granite model utility created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download and Setup Application Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the complete application code from the repository\n",
    "# This will copy all the necessary files for the HealthAssist AI application\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# For this demo, we'll create the essential files directly\n",
    "# In a real deployment, you would clone your repository or upload files\n",
    "\n",
    "# Create main application file\n",
    "app_content = '''\n",
    "import streamlit as st\n",
    "\n",
    "# Set page configuration first\n",
    "st.set_page_config(\n",
    "    page_title=\"HealthAssist AI\",\n",
    "    page_icon=\"🏥\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from utils.model_utils import IBMGraniteModelManager\n",
    "\n",
    "def initialize_session_state():\n",
    "    \"\"\"Initialize session state variables\"\"\"\n",
    "    if 'authenticated' not in st.session_state:\n",
    "        st.session_state.authenticated = False\n",
    "    if 'username' not in st.session_state:\n",
    "        st.session_state.username = None\n",
    "    if 'user_role' not in st.session_state:\n",
    "        st.session_state.user_role = None\n",
    "    if 'model_manager' not in st.session_state:\n",
    "        st.session_state.model_manager = None\n",
    "    if 'model_loaded' not in st.session_state:\n",
    "        st.session_state.model_loaded = False\n",
    "    if 'chat_history' not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "\n",
    "def login_page():\n",
    "    \"\"\"Display login page\"\"\"\n",
    "    st.title(\"🏥 HealthAssist AI\")\n",
    "    st.subheader(\"Powered by IBM Granite 3.3-2b-instruct\")\n",
    "    \n",
    "    col1, col2, col3 = st.columns([1, 2, 1])\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"### Login\")\n",
    "        \n",
    "        username = st.text_input(\"Username\")\n",
    "        password = st.text_input(\"Password\", type=\"password\")\n",
    "        \n",
    "        # Demo accounts\n",
    "        demo_accounts = {\n",
    "            \"admin\": {\"password\": \"admin123\", \"role\": \"admin\"},\n",
    "            \"doctor\": {\"password\": \"doctor123\", \"role\": \"doctor\"},\n",
    "            \"patient\": {\"password\": \"patient123\", \"role\": \"patient\"}\n",
    "        }\n",
    "        \n",
    "        if st.button(\"Login\", type=\"primary\"):\n",
    "            if username in demo_accounts and password == demo_accounts[username][\"password\"]:\n",
    "                st.session_state.authenticated = True\n",
    "                st.session_state.username = username\n",
    "                st.session_state.user_role = demo_accounts[username][\"role\"]\n",
    "                st.success(\"Login successful!\")\n",
    "                st.rerun()\n",
    "            else:\n",
    "                st.error(\"Invalid credentials\")\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"**Demo Accounts:**\")\n",
    "        st.markdown(\"- Admin: admin / admin123\")\n",
    "        st.markdown(\"- Doctor: doctor / doctor123\")\n",
    "        st.markdown(\"- Patient: patient / patient123\")\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load the IBM Granite model\"\"\"\n",
    "    if st.session_state.model_manager is None:\n",
    "        with st.spinner(\"Loading IBM Granite 3.3-2b-instruct model...\"):\n",
    "            st.session_state.model_manager = IBMGraniteModelManager()\n",
    "            success = st.session_state.model_manager.load_model()\n",
    "            st.session_state.model_loaded = success\n",
    "            \n",
    "            if success:\n",
    "                st.success(\"IBM Granite model loaded successfully!\")\n",
    "            else:\n",
    "                st.error(\"Failed to load model. Check GPU availability.\")\n",
    "            \n",
    "            st.rerun()\n",
    "\n",
    "def chat_interface():\n",
    "    \"\"\"Main chat interface\"\"\"\n",
    "    st.title(\"💬 AI Medical Assistant\")\n",
    "    \n",
    "    # Display chat history\n",
    "    for message in st.session_state.chat_history:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.write(message[\"content\"])\n",
    "    \n",
    "    # Chat input\n",
    "    if prompt := st.chat_input(\"Ask me about your health concerns...\"):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(prompt)\n",
    "        \n",
    "        # Generate AI response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            if st.session_state.model_loaded and st.session_state.model_manager:\n",
    "                with st.spinner(\"Thinking...\"):\n",
    "                    response = st.session_state.model_manager.medical_chat(\n",
    "                        prompt, \n",
    "                        st.session_state.chat_history[:-1]\n",
    "                    )\n",
    "                st.write(response)\n",
    "                \n",
    "                # Add assistant response to chat history\n",
    "                st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            else:\n",
    "                st.write(\"Please load the AI model first using the sidebar.\")\n",
    "\n",
    "def symptom_checker():\n",
    "    \"\"\"Symptom checker interface\"\"\"\n",
    "    st.title(\"🔍 AI Symptom Checker\")\n",
    "    \n",
    "    st.warning(\"⚠️ This tool is for informational purposes only. Always consult a healthcare professional for proper medical advice.\")\n",
    "    \n",
    "    with st.form(\"symptom_form\"):\n",
    "        symptoms = st.text_area(\"Describe your symptoms:\", height=100)\n",
    "        age = st.number_input(\"Age\", min_value=0, max_value=120, value=30)\n",
    "        duration = st.selectbox(\"How long have you had these symptoms?\", \n",
    "                               [\"Less than 1 day\", \"1-3 days\", \"1 week\", \"2+ weeks\", \"1+ months\"])\n",
    "        severity = st.slider(\"Pain/Discomfort level (0-10)\", 0, 10, 5)\n",
    "        \n",
    "        submitted = st.form_submit_button(\"Analyze Symptoms\")\n",
    "        \n",
    "        if submitted and symptoms:\n",
    "            if st.session_state.model_loaded and st.session_state.model_manager:\n",
    "                patient_info = f\"Age: {age}, Duration: {duration}, Severity: {severity}/10\"\n",
    "                \n",
    "                with st.spinner(\"Analyzing symptoms...\"):\n",
    "                    analysis = st.session_state.model_manager.symptom_analysis(symptoms, patient_info)\n",
    "                \n",
    "                st.subheader(\"Analysis Results\")\n",
    "                st.write(analysis)\n",
    "                \n",
    "                st.info(\"💡 Remember: This analysis is for informational purposes only. Please consult a healthcare professional for proper diagnosis and treatment.\")\n",
    "            else:\n",
    "                st.error(\"Please load the AI model first using the sidebar.\")\n",
    "\n",
    "def main_app():\n",
    "    \"\"\"Main application interface\"\"\"\n",
    "    # Sidebar\n",
    "    with st.sidebar:\n",
    "        st.title(\"🏥 HealthAssist AI\")\n",
    "        st.write(f\"Welcome, {st.session_state.username}!\")\n",
    "        st.write(f\"Role: {st.session_state.user_role.title()}\")\n",
    "        \n",
    "        # Model status and loading\n",
    "        if st.session_state.model_loaded:\n",
    "            st.success(\"🤖 IBM Granite Model Ready\")\n",
    "        else:\n",
    "            st.error(\"🤖 Model Not Loaded\")\n",
    "            if st.button(\"Load IBM Granite Model\"):\n",
    "                load_model()\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        \n",
    "        # Navigation\n",
    "        page = st.selectbox(\"Navigate to:\", \n",
    "                           [\"💬 AI Chat\", \"🔍 Symptom Checker\", \"📊 Wellness Dashboard\"])\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        \n",
    "        # Logout\n",
    "        if st.button(\"🚪 Logout\"):\n",
    "            st.session_state.authenticated = False\n",
    "            st.session_state.username = None\n",
    "            st.session_state.user_role = None\n",
    "            st.rerun()\n",
    "    \n",
    "    # Main content\n",
    "    if page == \"💬 AI Chat\":\n",
    "        chat_interface()\n",
    "    elif page == \"🔍 Symptom Checker\":\n",
    "        symptom_checker()\n",
    "    elif page == \"📊 Wellness Dashboard\":\n",
    "        st.title(\"📊 Wellness Dashboard\")\n",
    "        st.info(\"Wellness dashboard features will be available once the model is loaded.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main application entry point\"\"\"\n",
    "    initialize_session_state()\n",
    "    \n",
    "    if not st.session_state.authenticated:\n",
    "        login_page()\n",
    "    else:\n",
    "        main_app()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(app_content)\n",
    "\n",
    "print(\"✅ Main application file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Streamlit application with ngrok\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_streamlit():\n",
    "    \"\"\"Run Streamlit application\"\"\"\n",
    "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\"])\n",
    "\n",
    "# Start Streamlit in background\n",
    "streamlit_thread = threading.Thread(target=run_streamlit)\n",
    "streamlit_thread.daemon = True\n",
    "streamlit_thread.start()\n",
    "\n",
    "# Wait for Streamlit to start\n",
    "print(\"Starting Streamlit application...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"\\n🚀 HealthAssist AI is now running!\")\n",
    "print(f\"🌐 Public URL: {public_url}\")\n",
    "print(f\"\\n📝 Demo Login Credentials:\")\n",
    "print(f\"   Admin: admin / admin123\")\n",
    "print(f\"   Doctor: doctor / doctor123\")\n",
    "print(f\"   Patient: patient / patient123\")\n",
    "print(f\"\\n⚡ Features:\")\n",
    "print(f\"   - AI Medical Chat (powered by IBM Granite 3.3-2b-instruct)\")\n",
    "print(f\"   - Intelligent Symptom Analysis\")\n",
    "print(f\"   - Health Data Management\")\n",
    "print(f\"   - Role-based Access Control\")\n",
    "print(f\"\\n🔧 Next Steps:\")\n",
    "print(f\"   1. Click the public URL above\")\n",
    "print(f\"   2. Login with demo credentials\")\n",
    "print(f\"   3. Load the IBM Granite model from sidebar\")\n",
    "print(f\"   4. Start chatting with your AI health assistant!\")\n",
    "\n",
    "# Keep the application running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Stopping application...\")\n",
    "    ngrok.disconnect(public_url)\n",
    "    ngrok.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Monitor Application (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage and system resources\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.used,memory.free,temperature.gpu,utilization.gpu --format=csv\n",
    "\n",
    "# Check active tunnels\n",
    "tunnels = ngrok.get_tunnels()\n",
    "print(f\"\\nActive ngrok tunnels: {len(tunnels)}\")\n",
    "for tunnel in tunnels:\n",
    "    print(f\"- {tunnel.name}: {tunnel.public_url} -> {tunnel.config['addr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Getting Started:\n",
    "1. **Switch to GPU Runtime**: Runtime → Change runtime type → GPU (T4 recommended)\n",
    "2. **Run Setup Cells**: Execute cells 1-4 to install dependencies and create files\n",
    "3. **Get Ngrok Token**: Sign up at [ngrok.com](https://ngrok.com) for free auth token\n",
    "4. **Launch Application**: Run cell 5 to start the application\n",
    "5. **Access Your App**: Click the public URL provided\n",
    "\n",
    "### Features Available:\n",
    "- **AI Medical Chat**: Natural conversation with IBM Granite model\n",
    "- **Symptom Analysis**: Intelligent symptom checking and recommendations\n",
    "- **Multi-user Support**: Admin, Doctor, and Patient roles\n",
    "- **Health Management**: Comprehensive health data tracking\n",
    "\n",
    "### Model Performance:\n",
    "- **Memory Usage**: ~4-6GB VRAM with 4-bit quantization\n",
    "- **Response Time**: 2-5 seconds per query on T4 GPU\n",
    "- **Context Length**: Maintains conversation history\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Model Loading Issues**: Ensure GPU runtime is selected and restart if needed\n",
    "- **Memory Errors**: Try reducing max_length in model responses\n",
    "- **Connection Issues**: Regenerate ngrok tunnel if URL becomes inactive\n",
    "\n",
    "### Security Notes:\n",
    "- This is a demo deployment with simple authentication\n",
    "- For production use, implement proper user management and security\n",
    "- Never share real medical data in demo applications\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**: This application is for demonstration and educational purposes only. Always consult qualified healthcare professionals for medical advice, diagnosis, or treatment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}